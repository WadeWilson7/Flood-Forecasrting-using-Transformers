# -*- coding: utf-8 -*-
"""FloodForecastingFinal.ipynb

Automatically generated by Colab.

"""# Code"""

try:
    import google.colab
    COLAB = True
    print("Note: using Google CoLab")
except:
    print("Note: not using Google CoLab")
    COLAB = False

# Make use of a GPU or MPS (Apple) if one is available.  (see module 3.2)
import torch

device = (
    "mps"
    if getattr(torch, "has_mps", False)
    else "cuda"
    if torch.cuda.is_available()
    else "cpu"
)
print(f"Using device: {device}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from torch.optim.lr_scheduler import ReduceLROnPlateau
import os

file_address = os.getenv("FILE_ADDRESS")
df = pd.read_csv(file_address)

df.head()

df.info()

df.rename(columns = {"Hirakud inflow": "Hirakud_Inflow"}, inplace = True)

names = ["Date", "Col_1", "Basantpur", "Kelo", "Kurubhata", "Parampur", "Sundargarh", "Hirakud", "rainfall", "tavg"]

df.columns = names

df.describe()

(df["Hirakud"] == 0).sum()

"""There are 82 entries in total that have a value equal to 0"""

df1 = df.copy()

for i in range(1, df.shape[0] - 1):
  if(df1.iloc[i,7] == 0):
    df1.iloc[i,7] = (df1.iloc[i - 1, 7] + df1.iloc[i + 1, 7]) / 2

(df1["Hirakud"] == 0).sum()

df1.head()

df2 = df1.copy()

df2["Date"] = pd.to_datetime(df2["Date"])

df2.info()

df3 = df2.copy()

df3["Day"] = df3["Date"].dt.day
df3["Month"] = df3["Date"].dt.month
df3["Year"] = df3["Date"].dt.year

df3.iloc[1700:1705]

df4 = df3.iloc[:, 1:]

df4.head()

df4.describe()

df_train = df4[df4["Year"] < 2013]
df_test = df4[df4["Year"] > 2012]

df_train.tail()

df_test.head()

flood_train = df_train["Hirakud"].to_numpy().reshape(-1, 1)
flood_test = df_test["Hirakud"].to_numpy().reshape(-1, 1)

flood_train.shape

Scaler = StandardScaler()

flood_train = Scaler.fit_transform(flood_train).flatten().tolist()
flood_test = Scaler.fit_transform(flood_test).flatten().tolist()
#Normalized the datset using standard scalar

len(flood_train)

seq_size = 5

def to_sequences(seq_size, inflow):
    x = []
    y = []
    for i in range(len(inflow) - seq_size):
        window = inflow[i:(i + seq_size)]
        after_window = inflow[(i + seq_size)]
        x.append(window)
        y.append(after_window)
    return torch.tensor(x, dtype=torch.float32).view(-1, seq_size, 1), torch.tensor(y, dtype=torch.float32).view(-1, 1)

x_train, y_train = to_sequences(seq_size, flood_train) #the shape of the x_train is (1581, 5, 1)
x_test, y_test = to_sequences(seq_size, flood_test) #The shape of the y_train is (1581, 1)

x_train.shape

y_train.shape

train_dataset = TensorDataset(x_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

test_dataset = TensorDataset(x_test, y_test)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Positional Encoding for Transformer
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

np.random.seed(42)
# Model definition using Transformer
class TransformerModel(nn.Module):
    def __init__(self, input_dim=1, d_model=64, nhead=32, num_layers=2, dropout=0.2):
        super(TransformerModel, self).__init__()

        self.encoder = nn.Linear(input_dim, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout)
        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)
        self.decoder = nn.Linear(d_model, 1)

    def forward(self, x):
        x = self.encoder(x)
        x = self.pos_encoder(x)
        x = self.transformer_encoder(x)
        x = self.decoder(x[:, -1, :])
        return x

model = TransformerModel().to(device)

# Train the model
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)
scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)

epochs = 100
early_stop_count = 0
min_val_loss = float('inf')

for epoch in range(epochs):
    model.train()
    for batch in train_loader:
        x_batch, y_batch = batch
        x_batch, y_batch = x_batch.to(device), y_batch.to(device)

        optimizer.zero_grad()
        outputs = model(x_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()

    # Validation
    model.eval()
    val_losses = []
    with torch.no_grad():
        for batch in test_loader:
            x_batch, y_batch = batch
            x_batch, y_batch = x_batch.to(device), y_batch.to(device)
            outputs = model(x_batch)
            loss = criterion(outputs, y_batch)
            val_losses.append(loss.item())

    val_loss = np.mean(val_losses)
    scheduler.step(val_loss)

    if val_loss < min_val_loss:
        min_val_loss = val_loss
        early_stop_count = 0
    else:
        early_stop_count += 1

    if early_stop_count >= 5:
        print("Early stopping!")
        break
    print(f"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.4f}")

model.eval()
predictions = []
with torch.no_grad():
    for batch in test_loader:
        x_batch, y_batch = batch
        x_batch = x_batch.to(device)
        outputs = model(x_batch)
        predictions.extend(outputs.squeeze().tolist())

rmse = np.sqrt(np.mean((Scaler.inverse_transform(np.array(predictions).reshape(-1, 1)) - Scaler.inverse_transform(y_test.numpy().reshape(-1, 1)))**2))
print(f"Score (RMSE): {rmse:.4f}")

P = Scaler.inverse_transform(np.array(predictions).reshape(-1,1))
y_actual = Scaler.inverse_transform(y_test.numpy().reshape(-1,1))

P[150]

y_actual[150]

df_test.iloc[149,7]

df_test_2 = df_test.copy()
df_test_2 = df_test_2.reset_index(drop=True)

plt.plot(P)
plt.plot(y_actual)
# plt.plot(df_test_2["Hirakud"])

value = max(P)
indices = np.where(P == value)[0]
indices

P[185]

y_actual[185]

df_test_2["Hirakud"][190]

from sklearn.metrics import mean_squared_error, mean_absolute_error

def model_evaluation_metrics(observed, simulated):
    """
    Calculate various evaluation metrics for a model.

    Parameters:
    observed (list): List of observed values.
    simulated (list): List of simulated or model-predicted values.

    Returns:
    dict: Dictionary containing MSE, RMSE, MAE, and NSE metrics.
    """
    if len(observed) != len(simulated):
        raise ValueError("Length of observed and simulated data should be the same.")

    # Convert lists to numpy arrays for easier calculations
    observed = np.array(observed)
    simulated = np.array(simulated)

    # Mean Squared Error (MSE)
    mse = mean_squared_error(observed, simulated)

    # Root Mean Squared Error (RMSE)
    rmse = np.sqrt(mse)

    # Mean Absolute Error (MAE)
    mae = mean_absolute_error(observed, simulated)

    # Nash-Sutcliffe Efficiency (NSE)
    mean_observed = np.mean(observed)
    numerator = np.sum((observed - simulated) ** 2)
    denominator = np.sum((observed - mean_observed) ** 2)
    nse = 1 - (numerator / denominator)

    # Return metrics in a dictionary
    metrics = {
        'MSE': mse,
        'RMSE': rmse,
        'MAE': mae,
        'NSE': nse
    }

    return metrics

metrics_result = model_evaluation_metrics(y_actual, P)
print("Model Evaluation Metrics:")
for metric, value in metrics_result.items():
    print(f"{metric}: {value}")

